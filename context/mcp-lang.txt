This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-06T20:43:51.339Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
src/
  index.ts
  init-chat-model.ts
  load-config.ts
.env.template
.gitignore
eslint.config.js
LICENSE
llm-mcp-config.json5
package.json
README.md
tsconfig.json

================================================================
Files
================================================================

================
File: src/index.ts
================
// Copyright (C) 2024 Hideya Kawahara
// SPDX-License-Identifier: MIT

import { createReactAgent } from '@langchain/langgraph/prebuilt';
import { MemorySaver } from '@langchain/langgraph';
import { HumanMessage } from '@langchain/core/messages';
import { convertMCPServersToLangChainTools, MCPServerCleanupFunction } from '@h1deya/mcp-langchain-tools';
import { initChatModel } from './init-chat-model.js';
import { loadConfig, Config } from './load-config.js';
import readline from 'readline';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';
import dotenv from 'dotenv';

// Initialize environment variables
dotenv.config();

// Constants
const DEFAULT_CONFIG_PATH = './llm-mcp-config.json5';

const SAMPLE_QUERIES = [
  'Whats the weather like in SF?',
  'Read and briefly summarize the file ./LICENSE',
  'Read the news headlines on cnn.com',
  // 'Show me the page cnn.com',
] as const;

const COLORS = {
  YELLOW: '\x1b[33m',
  CYAN: '\x1b[36m',
  RESET: '\x1b[0m'
} as const;

// CLI argument setup
interface Arguments {
  config: string;
  [key: string]: unknown;
}

const parseArguments = (): Arguments => {
  return yargs(hideBin(process.argv))
    .options({
      config: {
        type: 'string',
        description: 'Path to config file',
        demandOption: false
      },
    })
    .help()
    .alias('help', 'h')
    .parseSync() as Arguments;
};

// Input handling
const createReadlineInterface = () => {
  return readline.createInterface({
    input: process.stdin,
    output: process.stdout
  });
};

const getInput = (rl: readline.Interface, prompt: string): Promise<string> => {
  return new Promise((resolve) => rl.question(prompt, resolve));
};

async function getUserQuery(
  rl: readline.Interface,
  remainingQueries: string[]
): Promise<string | undefined> {
  const input = await getInput(rl, `${COLORS.YELLOW}Query: `);
  process.stdout.write(COLORS.RESET);
  const query = input.trim();

  if (query.toLowerCase() === 'quit' || query.toLowerCase() === 'q') {
    rl.close();
    return undefined;
  }

  if (query === '') {
    const sampleQuery = remainingQueries.shift();
    if (!sampleQuery) {
      console.log('\nPlease type a query, or "quit" or "q" to exit\n');
      return await getUserQuery(rl, remainingQueries);
    }
    process.stdout.write('\x1b[1A\x1b[2K'); // Move up and clear the line
    console.log(`${COLORS.YELLOW}Sample Query: ${sampleQuery}${COLORS.RESET}`);
    return sampleQuery;
  }

  return query;
}

// Conversation loop
async function handleConversation(
  agent: ReturnType<typeof createReactAgent>,
  remainingQueries: string[]
): Promise<void> {
  console.log('\nConversation started. Type "quit" or "q" to end the conversation.\n');
  console.log('Sample Queries (type just enter to supply them one by one):');
  remainingQueries.forEach(query => console.log(`- ${query}`));
  console.log();

  const rl = createReadlineInterface();
 
  while (true) {
    const query = await getUserQuery(rl, remainingQueries);
    console.log();

    if (!query) {
      console.log(`${COLORS.CYAN}Goodbye!${COLORS.RESET}\n`);
      return;
    }

    const agentFinalState = await agent.invoke(
      { messages: [new HumanMessage(query)] },
      { configurable: { thread_id: 'test-thread' } }
    );

    const result = agentFinalState.messages[agentFinalState.messages.length - 1].content;

    console.log(`${COLORS.CYAN}${result}${COLORS.RESET}\n`);
  }
}

// Application initialization
async function initializeReactAgent(config: Config) {
  console.log('Initializing model...', config.llm, '\n');
  const llm = initChatModel(config.llm);

  console.log(`Initializing ${Object.keys(config.mcpServers).length} MCP server(s)...\n`);
  const { tools, cleanup } = await convertMCPServersToLangChainTools(
    config.mcpServers,
    { logLevel: 'info' }
  );

  const agent = createReactAgent({
    llm,
    tools,
    checkpointSaver: new MemorySaver(),
  });

  return { agent, cleanup };
}

// Main
async function main(): Promise<void> {
  let mcpCleanup: MCPServerCleanupFunction | undefined;

  try {
    const argv = parseArguments();
    const configPath = argv.config || process.env.CONFIG_FILE || DEFAULT_CONFIG_PATH;
    const config = loadConfig(configPath);

    const { agent, cleanup } = await initializeReactAgent(config);
    mcpCleanup = cleanup;

    await handleConversation(agent, [...SAMPLE_QUERIES]);

  } finally {
    if (mcpCleanup) {
      await mcpCleanup();
    }
  }
}

// Application entry point with error handling
main().catch((error: unknown) => {
  const errorMessage = error instanceof Error ? error.message : 'An unknown error occurred';
  console.error(errorMessage, error);
  process.exit(1);
});

================
File: src/init-chat-model.ts
================
// Copyright (C) 2024 Hideya Kawahara
// SPDX-License-Identifier: MIT

import { ChatOpenAI } from '@langchain/openai';
import { ChatAnthropic } from '@langchain/anthropic';
import { ChatGroq } from '@langchain/groq';
import { BaseChatModel } from '@langchain/core/language_models/chat_models';
import { Tool } from '@langchain/core/tools';

// FIXME: no typescript version of init_chat_model()? (or the Python version is gone?)
// Ref: https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html
// Ref: https://v03.api.js.langchain.com/classes/_langchain_core.language_models_chat_models.BaseChatModel.html

interface ChatModelConfig {
  provider: string;
  apiKey?: string;
  modelName?: string;
  temperature?: number;
  maxTokens?: number,
  tools?: Tool[];
}

export function initChatModel(config: ChatModelConfig): BaseChatModel {
  let model: BaseChatModel;

  // remove unnecessary properties
  // eslint-disable-next-line @typescript-eslint/no-unused-vars
  const { provider, tools, ...llmConfig } = config;

  try {
    switch (config.provider.toLowerCase()) {
      case 'openai':
        model = new ChatOpenAI(llmConfig);
        break;

      case 'anthropic':
        model = new ChatAnthropic(llmConfig);
        break;

      case 'groq':
        // somehow, the API key had to be set via the env variable,
        // even though the constructor accepts `apiKey`
        if (llmConfig.apiKey) {
          process.env.GROQ_API_KEY = llmConfig.apiKey;
        }
        model = new ChatGroq(llmConfig);
        break;

      default:
        throw new Error(
          `Unsupported provider: ${config.provider}`,
        );
    }

    if (typeof model?.bindTools === 'function') {
      if (config.tools && config.tools.length > 0) {
        // FIXME
        // eslint-disable-next-line @typescript-eslint/no-unsafe-function-type
        model = (model as { bindTools: Function }).bindTools(config.tools);
      }
    } else {
      throw new Error(
        `Tool calling unsupported by provider: ${config.provider}`,
      );
    }

    return model;
  } catch (error) {
    throw new Error(`Failed to initialize chat model: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
}

================
File: src/load-config.ts
================
// Copyright (C) 2024 Hideya Kawahara
// SPDX-License-Identifier: MIT

import JSON5 from 'json5';
import { readFileSync } from 'fs';

export interface LLMConfig {
  provider: string;
  modelName?: string;
  temperature?: number;
  maxTokens?: number;
}

export interface MCPServerConfig {
  command: string;
  args: string[];
  env?: Record<string, string>;
}

export interface Config {
  llm: LLMConfig;
  mcpServers: {
    [key: string]: MCPServerConfig;
  }
}

export function loadConfig(path: string): Config {
  try {
    let json5Str = readFileSync(path, 'utf-8');

    // Replace environment variables in the format ${VAR_NAME} with their values
    Object.entries(process.env).forEach(([key, value]) => {
      json5Str = json5Str.replace(`\${${key}}`, value || '');
    });

    const config = JSON5.parse(json5Str);

    // Validate required fields
    validateConfig(config);

    return config;
  } catch (error) {
    if (error instanceof Error) {
      throw new Error(`Failed to load configuration from "${path}": ${error.message}`);
    }
    throw error;
  }
}

// eslint-disable-next-line @typescript-eslint/no-explicit-any
function validateConfig(config: any): asserts config is Config {
  if (typeof config !== 'object' || config === null) {
    throw new Error('Configuration must be an object');
  }

  if (!config.llm) {
    throw new Error('LLM configuration is required');
  }
  validateLLMConfig(config.llm);

  if (typeof config.mcpServers !== 'object' || config.mcpServers === null) {
    throw new Error('mcpServers must be an object');
  }

  Object.entries(config.mcpServers).forEach(([key, value]) => {
    try {
      validateMCPServerConfig(value);
    } catch (error) {
      throw new Error(`Invalid configuration for MCP server "${key}": ${error instanceof Error ? error.message : String(error)}`);
    }
  });
}

// eslint-disable-next-line @typescript-eslint/no-explicit-any
function validateLLMConfig(llmConfig: any): asserts llmConfig is LLMConfig {
  if (typeof llmConfig !== 'object' || llmConfig === null) {
    throw new Error('LLM configuration must be an object');
  }

  if (typeof llmConfig.provider !== 'string') {
    throw new Error('LLM provider must be a string');
  }

  if (llmConfig.modelName !== undefined && typeof llmConfig.modelName !== 'string') {
    throw new Error('LLM modelName must be a string if provided');
  }

  if (llmConfig.temperature !== undefined && typeof llmConfig.temperature !== 'number') {
    throw new Error('LLM temperature must be a number if provided');
  }

  if (llmConfig.maxTokens !== undefined && typeof llmConfig.maxTokens !== 'number') {
    throw new Error('LLM maxTokens must be a number if provided');
  }
}

// eslint-disable-next-line @typescript-eslint/no-explicit-any
function validateMCPServerConfig(serverConfig: any): asserts serverConfig is MCPServerConfig {
  if (typeof serverConfig !== 'object' || serverConfig === null) {
    throw new Error('MCP server configuration must be an object');
  }

  if (typeof serverConfig.command !== 'string') {
    throw new Error('MCP server command must be a string');
  }

  if (!Array.isArray(serverConfig.args)) {
    throw new Error('MCP server args must be an array');
  }

  if (serverConfig.args.some((arg: unknown) => typeof arg !== 'string')) {
    throw new Error('All MCP server args must be strings');
  }

  if (serverConfig.env !== undefined) {
    if (typeof serverConfig.env !== 'object' || serverConfig.env === null) {
      throw new Error('MCP server env must be an object if provided');
    }

    // Validate that all env values are strings
    for (const [, value] of Object.entries(serverConfig.env)) {
      if (typeof value !== 'string') {
        throw new Error('All MCP server env values must be strings');
      }
    }
  }
}

================
File: .env.template
================
# CONFIG_FILE=./llm-mcp-config.json5

OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
GROQ_API_KEY=gsk_...

# BRAVE_API_KEY=BSA...

================
File: .gitignore
================
.env
node_modules/
.DS_Store

================
File: eslint.config.js
================
import eslint from '@eslint/js';
import tseslint from 'typescript-eslint';

export default tseslint.config(
  eslint.configs.recommended,
  ...tseslint.configs.recommended,
  {
    languageOptions: {
      parser: tseslint.parser,
      parserOptions: {
        project: './tsconfig.json',
      },
    },
    files: ['**/*.ts', '**/*.tsx'],
  }
);

================
File: LICENSE
================
MIT License

Copyright (c) 2024 hideya

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: llm-mcp-config.json5
================
//
// The configuration file format is [JSON5](https://json5.org/),
// where comments and trailing commas are allowed.
// The file format is further extended to replace `${...}` notations
// with the values of appropriate environment variables.
// Put all the credentials and private into the `.env` file
// and refer to them with `${...}` notation if necessary.
//
{
    "llm": {
        "provider": "openai",
        "modelName": "gpt-4o-mini",
        "temperature": 0.0,
        "mexTokens": 1000,
    },
    // "llm": {
    //     "provider": "anthropic",
    //     "modelName": "claude-3-5-haiku-latest",
    //     "temperature": 0.0,
    //     "mexTokens": 1000,
    // },
    // "llm": {
    //     "provider": "groq",
    //     "modelName": "mixtral-8x7b-32768",
    //     "temperature": 0.0,
    //     "mexTokens": 1000,
    // },
    "mcpServers": {
        // https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem
        "filesystem": {
            "command": "npx",
            "args": [
                "-y",
                "@modelcontextprotocol/server-filesystem",
                "."
            ]
        },
        // https://github.com/modelcontextprotocol/servers/tree/main/src/fetch
        "fetch": {
            "command": "uvx",
            "args": ["mcp-server-fetch"]
        },
        // https://github.com/modelcontextprotocol/quickstart-resources/tree/main/weather-server-python
        "weather": {
            "command": "npx",
            "args": [
                "-y",
                "@h1deya/mcp-server-weather"
            ]

        },
        // // https://github.com/modelcontextprotocol/servers/tree/main/src/puppeteer
        // "puppeteer": {
        //     "command": "npx",
        //     "args": ["-y", "@modelcontextprotocol/server-puppeteer"]
        // },
        // // https://github.com/modelcontextprotocol/servers/tree/main/src/brave-search
        // "brave-search": {
        //     "command": "npx",
        //     "args": [
        //         "-y",
        //         "@modelcontextprotocol/server-brave-search"
        //     ],
        //     "env": {
        //         "BRAVE_API_KEY": "${BRAVE_API_KEY}",
        //         "PATH": "${PATH}" // Somehow, PATH need to be explicitly specified here
        //     }
        // },
    }
}

================
File: package.json
================
{
  "type": "module",
  "scripts": {
    "start": "tsx src/index.ts",
    "dev": "tsx watch src/index.ts",
    "lint": "eslint src"
  },
  "dependencies": {
    "@h1deya/mcp-langchain-tools": "^0.0.7",
    "@langchain/anthropic": "^0.3.11",
    "@langchain/core": "^0.3.27",
    "@langchain/groq": "^0.1.2",
    "@langchain/langgraph": "^0.2.39",
    "@langchain/openai": "^0.3.16",
    "@types/yargs": "^17.0.33",
    "dotenv": "^16.4.7",
    "json5": "^2.2.3",
    "yargs": "^17.7.2"
  },
  "devDependencies": {
    "@eslint/js": "^9.17.0",
    "@types/node": "^22.10.5",
    "@typescript-eslint/eslint-plugin": "^8.19.0",
    "@typescript-eslint/parser": "^8.19.0",
    "eslint": "^9.17.0",
    "tsx": "^4.19.2",
    "typescript": "^5.7.2",
    "typescript-eslint": "^8.19.0"
  }
}

================
File: README.md
================
# MCP Client Implementation Using LangChain / TypeScript [![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/hideya/react/blob/main/LICENSE)

This simple MCP-client demonstrates
[Model Context Protocol](https://modelcontextprotocol.io/) server invocations from
LangChain ReAct Agent by wrapping MCP server tools into LangChain Tools.

It leverages a utility function `convertMCPServersToLangChainTools()`
from [`@h1deya/mcp-langchain-tools`](https://www.npmjs.com/package/@h1deya/mcp-langchain-tools)

LLMs from OpenAI, Anthropic and Groq are currently supported.

## Setup
1. Install dependencies:
    ```bash
    npm install
    ```

2. Setup API keys:
    ```bash
    cp .env.template .env
    ```
    - Update `.env` as needed.
    - `.gitignore` is configured to ignore `.env`
      to prevent accidental commits of the credentials.

3. Configure LLM and MCP Servers settings `llm-mcp-config.json5` as needed.

    - The configuration file format for MCP servers is the same as
      [Claude for Desktop](https://modelcontextprotocol.io/quickstart/user) 
    - However, the format used for this app is more flexible [JSON5](https://json5.org/),
      which allows comments and trailing commas.
    - The file format is also extended to
      replace `${...}` notations with the values of corresponding environment variables.
    - Keep all the credentials and private info in the `.env` file
      and refer to them with `${...}` notation as needed.


## Usage
Development (watch) mode:
```bash
npm run dev
```
Regular mode:
```bash
npm start
```

At the prompt, you can simply press Enter to use sample queries that perform MCP server tool invocations.

================
File: tsconfig.json
================
{
    "compilerOptions": {
        "target": "ES2022",
        "module": "Node16",
        "moduleResolution": "Node16",
        "outDir": "./build",
        "rootDir": "./src",
        "strict": true,
        "esModuleInterop": true,
        "skipLibCheck": true,
        "forceConsistentCasingInFileNames": true
    },
    "include": ["src/**/*"]
}
